# Process-Pager A/B Results (VM) — `20260225-162140`

This document summarizes the repeated A/B benchmark run for the cooperative
`process-pager` prototype.

## What This Experiment Is

For each supported workload, we ran:

1. **Baseline**: normal workload execution (no process-pager)
2. **Pager**: cooperative `process-pager` enabled
   - DAMON-based cold classification
   - page compression in daemon
   - cooperative client-local eviction (`madvise(MADV_DONTNEED)`)
   - `userfaultfd` missing-fault restore on access

The benchmark compares workload-level throughput impact and pager-internal
overheads/latencies.

## Run Configuration

- **Run ID**: `20260225-162140`
- **Result directory**: `workloads/results/process-pager-ab-repeats/20260225-162140`
- **Repeats**: `5`
- **Duration per run**: `10s`
- **Environment**: `predicomp-vm` (Arch VM)
- **Kernel**: `6.18.9-arch1-2`

Manifest:
- `workloads/results/process-pager-ab-repeats/20260225-162140/manifest.json`

Aggregate outputs:
- `workloads/results/process-pager-ab-repeats/20260225-162140/all_runs_long.csv`
- `workloads/results/process-pager-ab-repeats/20260225-162140/aggregate.csv`
- `workloads/results/process-pager-ab-repeats/20260225-162140/aggregate_summary.txt`

Plots:
- `workloads/results/process-pager-ab-repeats/20260225-162140/plots`

## Workloads Included

- `interactive_burst`
- `anon_streamer`
- `random_touch_heap`
- `mmap_churn`

These are the workloads currently supported by the cooperative process-pager path
(actual workload pages, not sidecars).

## Important Environment Notes

- The VM kernel rejected remote `process_madvise(MADV_DONTNEED)` from the daemon.
- The prototype therefore used the **cooperative client-eviction fallback** for
  stable-region workloads (`interactive_burst`, `anon_streamer`, `random_touch_heap`).
- GNU `time` was not installed at benchmark time, so `time -v` host-side columns
  in the A/B outputs are `0`. This does not affect pager daemon metrics.
- Plot generation was later enabled in the VM by installing `python-matplotlib`.

## Metric Semantics (Important)

### Throughput / Runtime

- `units_per_sec_ratio_pager_vs_baseline`
  - Primary workload impact metric (`pager / baseline`)
  - `< 1.0` means the pager reduces throughput
  - `> 1.0` can happen due to measurement noise/jitter and should not be treated
    as a guaranteed speedup without deeper validation

- `elapsed_ms_ratio_pager_vs_baseline`
  - Less informative here for fixed-duration workloads
  - Most workloads are configured to run for a target duration, so elapsed time
    stays near constant and this ratio remains near `1.0`

### Latency (Pager-side)

- `pager_fault_missing_p95_ns` / `pager_fault_missing_p99_ns`
  - End-to-end **fault-service latency** in the pager daemon for missing faults
  - Best proxy in this prototype for user stall when touching a compressed page

- `pager_restore_wall_p95_ns` / `pager_restore_wall_p99_ns`
  - **Restore path latency** (decompress + restore handling path)
  - Narrower than total fault-service latency

Interpretation rule:
- If missing-fault latency tails are much worse than restore latency tails, the
  decompressor is not the only bottleneck; scheduling/fault-path overhead is
  dominating tail behavior.

### Pager CPU

- `pager_daemon_cpu_pct_total`
  - Total pager daemon CPU as a percentage of session wall time
- `pager_bg_cpu_pct`
  - Background compression/classification thread contribution
- `pager_fault_cpu_pct`
  - Fault-handling thread contribution

### Pager Activity

- `pager_compress_success`
  - Pages successfully compressed by pager
- `pager_faults_missing`
  - Missing faults handled
- `pager_restore_success`
  - Successful page restores
- `pager_client_evict_success`
  - Successful cooperative client-local evictions

### Dynamic Range Metrics (`mmap_churn`)

- `pager_range_add_msgs`, `pager_range_del_msgs`
  - Runtime range registration/unregistration operations
- `pager_pages_tracked_peak`
  - Peak number of tracked pages in pager page table

## Aggregate Results (Median of 5 Runs)

| Workload | Throughput Ratio (`pager/base`) | Pager CPU % | BG CPU % | Fault CPU % | Missing p99 | Restore p99 | Compress Success | Missing Faults | Restores |
|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|
| `anon_streamer` | `0.999284` | `3.506%` | `3.349%` | `0.018%` | `27.184 us` | `17.689 us` | `104` | `104` | `104` |
| `interactive_burst` | `1.025478` | `2.036%` | `1.968%` | `0.013%` | `57.208 us` | `18.594 us` | `64` | `56` | `56` |
| `random_touch_heap` | `1.005344` | `4.673%` | `4.503%` | `0.036%` | `6.255 ms` | `31.434 us` | `296` | `117` | `117` |
| `mmap_churn` | `0.687105` | `12.129%` | `0.123%` | `11.274%` | `5.771 us` | `0` | `0` | `245632` | `0` |

## Workload-by-Workload Discussion

### `anon_streamer`

Observed:
- Throughput impact is effectively zero (`~0.999x`)
- Pager daemon CPU is modest (`~3.5%`) and mostly background thread work
- Missing-fault p99 and restore p99 are both in the tens of microseconds
- Compression / fault / restore counts align (`104 / 104 / 104`)

Interpretation:
- This workload is a strong fit for the current pager prototype.
- It provides a stable, sequential access pattern where cold-page compression and
  later restores happen in a predictable way.
- It is suitable for policy tuning and CPU-vs-latency tradeoff experiments.

### `interactive_burst`

Observed:
- Throughput ratio median is slightly above `1.0` (`~1.025x`)
- Pager daemon CPU is low (`~2.0%`)
- Missing-fault p99 (`~57 us`) is higher than `anon_streamer`, while restore p99
  stays moderate (`~19 us`)
- Compression and restore counts are lower than streamer, as expected from the
  burst/idle pattern and workload size

Interpretation:
- The `>1.0` throughput ratio should be treated as noise/jitter, not a real
  speedup claim.
- `interactive_burst` remains the best phase-structured workload for evaluating
  predictive colding/decompression policies because it has clear active/idle
  windows and repeated transitions.

### `random_touch_heap`

Observed:
- Throughput ratio remains near baseline (`~1.005x`)
- Pager daemon CPU is higher than `interactive_burst` and `anon_streamer`
  (`~4.67%`)
- **Missing-fault p99 is very large** (`~6.25 ms` median)
- Restore p99 remains small (`~31 us`)

Interpretation:
- This workload exposes tail-latency risk.
- The large gap between:
  - `missing-fault p99` (`ms`)
  - `restore p99` (`us`)
  indicates tail latency is not dominated by decompression codec work.
- Fault-path scheduling/timing effects dominate the worst-case stalls.

Research implication:
- `random_touch_heap` is the correct workload for stress-testing tail behavior
  and deciding how conservative cold-page selection must be.

### `mmap_churn`

Observed:
- Throughput ratio drops significantly (`~0.687x`)
- Pager daemon CPU is highest (`~12.1%`)
- CPU cost is overwhelmingly in the **fault thread** (`~11.27%`)
- `compress_success = 0`, `restore_success = 0`
- Missing faults are extremely high (`~245k`)
- Dynamic range add/del counts are high (median `1919` each)
- Peak tracked pages stay small (`128`)

Interpretation:
- `mmap_churn` successfully validates the **dynamic range registration/unregistration**
  path and the daemon’s ability to track rapidly changing mappings.
- It does **not** show compression benefit under these settings because mappings are
  short-lived and usually disappear before they become cold enough to compress.
- This workload is best treated as a **robustness and control-path stress test**
  for the pager, not a performance/compression win benchmark.

## Cross-Workload Findings

### 1) Pager overhead is workload-dependent, but generally modest for stable regions

For stable-region workloads:
- `interactive_burst`: `~2%` pager CPU
- `anon_streamer`: `~3.5%`
- `random_touch_heap`: `~4.7%`

This is a reasonable cost range for the current prototype, especially given it is
still research code and not optimized.

### 2) Tail latency is the primary risk, not average throughput

The most important result in this run is not throughput ratio but latency shape:
- `random_touch_heap` has acceptable average throughput but poor missing-fault tail
- restore latency remains small, so codec work is not the tail culprit

This supports focusing future optimization on:
- fault handling path behavior
- scheduling interactions
- policy conservatism for random-access workloads

### 3) `elapsed_ms` ratios are not very informative for fixed-duration workloads

All workloads are configured to run for a target duration, so elapsed time remains
nearly constant. Throughput (`units/sec`) is the right metric for workload impact
in this harness.

### 4) `mmap_churn` should not be mixed into compression-efficiency comparisons

`mmap_churn` is useful and relevant, but for a different question:
- “Does the dynamic range/fault path stay correct under mapping churn?”
not:
- “How much memory compression benefit do we get?”

## Plots and How to Read Them

Generated under:
- `workloads/results/process-pager-ab-repeats/20260225-162140/plots`

Primary plots:

- `throughput_ratio_by_workload.*`
  - Use this for top-line workload impact

- `pager_daemon_cpu_breakdown.*`
  - Distinguishes background compression work from fault-path cost
  - `mmap_churn` should stand out as fault-thread dominated

- `fault_missing_latency_p95_p99.*`
  - Main fault-tail comparison across workloads (log scale)

- `restore_latency_p95_p99.*`
  - Decompress/restore path tails (log scale)
  - Compare directly with missing-fault tails to isolate non-codec overhead

- `pager_activity_counts.*`
  - Quick sanity check that pager is actually active for each workload

- `mmap_churn_range_ops_vs_faults.*`
  - Validates dynamic range updates and explains why compression stays zero

- `pager_cpu_vs_throughput_ratio.*`
  - Compact cost-vs-impact view

## Limitations of This Result Set

1. **GNU `time` metrics unavailable during the benchmark run**
- Host-side `time -v` fields are zero in this dataset.
- Pager daemon CPU/latency metrics are still valid.

2. **5 repeats is enough for a first comparison, not for deep tail confidence**
- Tail latency estimates (especially `random_touch_heap`) will be more stable with
  more repeats and longer runs.

3. **Prototype semantics**
- Cooperative target model
- Single-target daemon session
- Client-local eviction fallback used (not remote `process_madvise`)

These are expected constraints at this stage.

## Recommended Next Runs

### Performance-focused comparison set

Run only stable-region workloads:
- `interactive_burst`
- `anon_streamer`
- `random_touch_heap`

Example:

```bash
./workloads/scripts/run_process_pager_ab_repeats.sh \
  --runs 10 \
  --duration-sec 20 \
  --only interactive_burst,anon_streamer,random_touch_heap
```

### Robustness-focused `mmap_churn` set

Keep `mmap_churn` separate to analyze:
- dynamic range add/del rates
- fault pressure
- pager fault-thread CPU cost

Example:

```bash
./workloads/scripts/run_process_pager_ab_repeats.sh \
  --runs 10 \
  --duration-sec 20 \
  --only mmap_churn
```

## Bottom Line

The cooperative process-pager prototype is now producing useful A/B data:

- **Stable-region workloads** (`anon_streamer`, `interactive_burst`) show low pager
  CPU overhead and microsecond-scale fault/restore tails.
- **Random access** (`random_touch_heap`) exposes the real challenge: **tail stall
  latency**, even when decompression itself remains fast.
- **Mapping churn** (`mmap_churn`) validates the dynamic range control/fault path,
  but should be treated as a robustness workload rather than a compression-benefit
  benchmark.

